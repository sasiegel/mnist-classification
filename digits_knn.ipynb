{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Digits with K-Nearest-Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple implementation of classifying images using the k-nearest-neighbors algorithm. The accuracy is pretty good for how simple the algorithm is. The parameters can be tinkered with but at the time of writing I am using k = 5, training data size = 10000, testing data size = 1000. Let's set these parameters, read in the data, then view one of the images and the label associated with it. Afterwards I'll explain the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "batch_size_train = 10000\n",
    "batch_size_test = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mnist = torchvision.datasets.MNIST('C:/projects/summer2020/vision/digits/', train=True, download=True,\n",
    "                        transform=torchvision.transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(train_mnist, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_mnist = torchvision.datasets.MNIST('C:/projects/summer2020/vision/digits/', train=False, download=True,\n",
    "                        transform=torchvision.transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(test_mnist,batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = enumerate(train_loader)\n",
    "_, (train_imgs, train_targets) = next(train_set)\n",
    "test_set = enumerate(test_loader)\n",
    "_, (test_imgs, test_targets) = next(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], <a list of 0 Text yticklabel objects>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_imgs[0][0], cmap='gray', interpolation='none')\n",
    "plt.title(\"Ground Truth: {}\".format(train_targets[0]))\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-nearest-neighbors algorithm is not very efficient and my implementation is even less efficient. I was aiming for simplicity over efficiency. We loop through each test image and find the distance to every training image. Distance is measured as Euclidean (p=2). We take the k nearest images and record the ground truth digit corresponding with the image. The predicted label is based on the majority of labels from k nearest images. The majority I chose to use is the median. It is very basic in that it is the central value/label. The effectiveness of this method of majority depends on our value of k. We compare the prediction with the ground truth of the test set which produces our prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = test_imgs.shape[0]\n",
    "n_train = train_imgs.shape[0]\n",
    "pred_test_targets = torch.zeros_like(test_targets)\n",
    "for i in range(n_test):\n",
    "    test_img = test_imgs[i]\n",
    "    distances = [torch.dist(test_img, train_imgs[j], p=2) for j in range(n_train)]\n",
    "    nearest_indices = np.array(distances).argsort()[:5]\n",
    "    pred_test_targets[i] = train_targets[nearest_indices].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy: 0.959\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.divide(sum(pred_test_targets == test_targets), len(test_targets))\n",
    "print('Prediction accuracy: {}'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
